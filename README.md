# Analysing the large Wikipedia dump dataset using Distributed Computing Techniques on GCP

Wikipedia offers free copies of all its available content, and the English-language Wikipedia dump dataset is over 19 GB compressed (expands to over 86 GB when decompressed). In this project I downloaded a specific Wikipedia data dump file, enwiki-latest-pages-articles1.xml-p1p41242.bz2, from the Wikimedia website and prepared it by downloading, decompressing and loading it into Spark RDDs and DataFrames for efficient data processing, and provided valuable insights into Wikipedia content and contributor behavior.

#### Credit
This is my code from an individual academic assignment given under the LSE ST446 coursework (2024); my work achieved the mark of **Distinction** in the assignment.
