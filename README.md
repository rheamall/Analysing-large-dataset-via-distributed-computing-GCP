# Analysing the English-language Wikipedia dump dataset using Distributed Computing Techniques on GCP

Wikipedia offers free copies of all its available content, and the English-language Wikipedia dump dataset is over 19 GB compressed (expands to over 86 GB when decompressed). In this project I prepared the Wikipedia dump dataset by downloading, decompressing and loading it into Spark RDDs and DataFrames for efficient data processing, and provided valuable insights into Wikipedia content and contributor behavior.

#### Credit
This is my code from an individual academic assignment given under the LSE ST446 coursework (2024); my work achieved the mark of **Distinction** in the assignment.
